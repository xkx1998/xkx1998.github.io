---
layout:     post
title:      Java并发容器和框架
subtitle:   
date:       2019-3-30
author:     BY xukexiang
header-img: img/charlotte/b2f68e7ebd6314a8358661a765ca9095527eeee1.jpg
catalog: true
tags:
    - Typora
---

## ConcurrentHashMap

### 1.为什么要使用ConcurrentHashMap
1、HashMap是线程不安全的

1) put操作可能会导致元素丢失
2) HashMap在扩容的时候可能会形成环形链表，造成死循环。

2、HashTable效率很低

1) HashTable是用synchronized来保证线程安全的

3、ConcurrentHashMap的锁分段技术可以有效提升并发的访问率
ConcurrentHashMap可以做到读取数据不加锁，并且其内部的结构可以让其在进行写操作的时候能够将锁的粒度保持地尽量地小，
允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对hash表的不同部分进行的修改。
ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的Hashtable，它们有自己的锁。
只要多个修改操作发生在不同的段上，它们就可以并发进行。

### 2.ConcurrentHashMap 的结构

- 一个ConcurrentHashMap包含一个Segment数组;
- 一个Segment里面包含多个HashEntry链表.

Segment是一种可重入锁, 在ConcurrentHashMap中扮演锁的角色. 
Segment的结构和HashMap的结构相似。
HashEntry则是实际存储键值对数据的地方. 当对HashEntry的数据进行修改时,
 必须首先获得与它对应的Segment锁.
![ConcurrentHashMap的内部结构](/img/1553929760(1).jpg)

### 3.ConcurrentHashMap的初始化

#### 初始化segments数组

```java
//concurrencyLevel决定segments的长度
if (concurrencyLevel > MAX_SEGMENTS) { // 如果自定义的concurrencyLevel超过允许最大的并发数
    concurrencyLevel = MAX_SEGMENTS; // 设定为允许的最大的并发数
}
int sshift = 0;
int ssize = 1;
while (ssize < concurrencyLevel) {
    ++sshift;
    // 为了能够使用按位与的散列算法来定位segments数组的索引, 必须保证segments数组的长度是2的N次方
    // 例如concurrencyLevel是14, 15或者16, 那么锁的个数也总是16
    ssize <<= 1;
}
segmentShift = 32 -sshift; // 用于定位参与散列运算的位数长度
segmentMask = ssize - 1; // 散列运算的掩码, 各个二进制位都是1
this.segments = Segment.newArray(ssize); // 创建Segment数组
```


CurrentHashMap的初始化一共有三个参数，一个initialCapacity，
表示初始的容量，一个loadFactor，表示负载参数，
最后一个是concurrentLevel，
代表ConcurrentHashMap内部的Segment的数量，ConcurrentLevel一经指定，不可改变，
后续如果ConcurrentHashMap的元素数量增加导致ConrruentHashMap需要扩容，
ConcurrentHashMap不会增加Segment的数量，
而只会增加Segment中链表数组的容量大小，
这样的好处是扩容过程不需要对整个ConcurrentHashMap做rehash，
而只需要对Segment里面的元素做一次rehash就可以了。

整个ConcurrentHashMap的初始化方法还是非常简单的，
先是根据concurrentLevel来new出Segment，
这里Segment的数量是不大于concurrentLevel的最大的2的指数
，就是说Segment的数量永远是2的指数个，
这样的好处是方便采用移位操作来进行hash，
加快hash的过程。接下来就是根据intialCapacity确定Segment的容量的大小，
每一个Segment的容量大小也是2的指数，同样使为了加快hash的过程。

这边需要特别注意一下两个变量，分别是segmentShift和segmentMask，
这两个变量在后面将会起到很大的作用，假设构造函数确定了Segment的数量是2的n次方，
那么segmentShift就等于32减去n，而segmentMask就等于2的n次方减一。


#### 初始化每个segment

```java
if (initialCapacity > MAXIMUM_CAPACITY) { // 如果自定义的Capacity超过了单个segment允许的最大的Capacity
    initialCapacity = MAXIMUM_CAPACITY; // 设定为单个segment允许的最大的Capacity
}
int c = initialCapacity / ssize; // 计算单个segment下的HashEntry数组的长度基值
if (c * ssize < initialCapacity) { // 确保数组能够容纳所有键值对
    ++c;
}
int cap = 1;
while (cap < c) {
    cap <<= 1; // 将HashEntry的长度向2的N次方对齐
}
for (int i = 0; i < this.segments.length; ++i) {
    // 初始化各个segment元素, 实际上是创建cap长度的HashEntry数组, loadFactor指定了HashEntry数组总体上允许的最大负载百分比
    this.segments[i] = new Segment<K, V>(cap, loadFactor)
}
```

单个segment的容量threshold=(int)cap*loadFactor;默认情况下initialCapatity等于16,loadfactor等于0.75,,通过运算
cap等于1，threshold等于0.


#### 定位Segment

在插入和获取元素的时候, 必须先通过散列算法定位到Segment, 
ConcurrentHashMap首先使用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再散列.

```java
// 本段代码无需理解, 只需要知道是一个对要插入ConcurrentHashMap元素的原始hash值的再散列(hash)的过程即可
private static int hash(int h) {
    h += (h << 15) ^ 0xffffcd7d;
    h ^= (h >>> 10);
    h += (h << 3);
    h ^= (h >>> 6);
    h += (h << 2) + (h << 14);
    return h ^ (h >>> 16);
}
```

```java
// 用再散列后的元素的hash值来定位segment数组索引
final Segment<K, V> segmentFor(int hash) { // hash 是再散列后的元素的hash值
    // 使用hash值的高四位进行Segment数组索引的计算
    // 这里假设segmentMask为1111, segmentShift为28, 也就是Segment数组的长度为16
    return segments[(hash >>> segmetnShift) & segmentMask]; 
}
```

之所以进行再散列, 目的是减少散列冲突, 使元素能够均匀分布在不同的Segment上. 如若负载不均, 那么分段锁就会失去意义.


#### ConcurrentHashMap的操作

##### get操作

步骤如下:

1) 对元素的原始hash值先进行再散列;
2) 通过这个散列值定位到segment数组索引;
3) 通过散列算法定位到具体的HashEntry数组中的元素;

```java
V get(Object key, int hash) {
    if (count != 0) { // read-volatile
        HashEntry<K,V> e = getFirst(hash);
        while (e != null) {
            if (e.hash == hash && key.equals(e.key)) {
                V v = e.value;
                if (v != null)
                    return v;
                return readValueUnderLock(e); // recheck
            }
            e = e.next;
        }
    }
    return null;
}
```

get的高效之处在于整个get过程不需要加锁, 因为value和count都是volatile类型的值. 
对volatile字段的写入操作happens-before于每一个后续的同一个字段的读操作。
因为实际上put、remove等操作也会更新count的值，所以当竞争发生的时候，
volatile的语义可以保证写操作在读操作之前，也就保证了写操作对后续的读操作都是可见的，这样后面get的后续操作就可以拿到完整的元素内容。
所以可以多线程并发读, 但是写入仍然是独占的(对每个数据段独占).

在确定了链表的头部以后，就可以对整个链表进行遍历，看第4行，取出key对应的value的值，
如果拿出的value的值是null，则可能这个key，value对正在put的过程中，如果出现这种情况，
那么就加锁来保证取出的value是完整的，如果不是null，则直接返回value。



##### put操作


```java
V put(K key, int hash, V value, boolean onlyIfAbsent) {
    lock();
    try {
        int c = count;
        if (c++ > threshold) // ensure capacity
            rehash();
        HashEntry<K,V>[] tab = table;
        int index = hash & (tab.length - 1);
        HashEntry<K,V> first = tab[index];
        HashEntry<K,V> e = first;
        while (e != null && (e.hash != hash || !key.equals(e.key)))
            e = e.next;
  
        V oldValue;
        if (e != null) {
            oldValue = e.value;
            if (!onlyIfAbsent)
                e.value = value;
        }
        else {
            oldValue = null;
            ++modCount;
            tab[index] = new HashEntry<K,V>(key, hash, first, value);
            count = c; // write-volatile
        }
        return oldValue;
    } finally {
        unlock();
    }
}
```

put操作是需要加锁的. 需要完成以下操作:

1) 通过再散列的hash值定位到需要插入的Segment;
2) 判断是否需要对Segment的HashEntry数组进行扩容(通过判断是否超过负载因子*总容量来判断);
3) 定位添加元素的位置;

在扩容的时候, 首先会在当前需要扩容的Segment中创建一个容量是之前两倍的数组, 然后将原数组里的元素进行再散列后插入到新的数组里.

为了高效, ConcurrentHashMap不会对整个容器进行扩容, 仅会对某个segment进行扩容.


#### size操作
size操作
size操作获取count的累加值的具体步骤是:

先通过两次不锁住segment数组的方式来累加count;
如果统计的过程中, 容器的count出现了变化, 则再采用加锁的方式统计Segment数组中所有元素的个数.
上述的判断容器的count是否发生变化是通过一个modCount变量来统计的, 任何会造成count变化的操作, 
比如put, remove, clean方法操作元素都会将modCount加1, 所以在进行size操作前后比较modCount元素是否发生变化, 就可以得知容器的大小是否发生变化.


## ConcurrentLinkedQueue

### 1.ConcurrentLinkedQueue的结构

#### 入队

从源代码角度来看整个入队过程主要做两件事情：

1. 第一是定位出尾节点

2. 第二是使用CAS算法能将入队节点设置成尾节点的next节点，如不成功则重试。

#### tail节点不一定为尾节点的设计意图

让tail节点永远作为队列的尾节点，这样实现代码量非常少，而且逻辑非常清楚和易懂。
但是这么做有个缺点就是每次都需要使用循环CAS更新tail节点。如果能减少CAS更新tail节点的次数，就能提高入队的效率。

在JDK 1.7的实现中，doug lea使用hops变量来控制并减少tail节点的更新频率，
并不是每次节点入队后都将 tail节点更新成尾节点，
而是当tail节点和尾节点的距离大于等于常量HOPS的值（默认等于1）时才更新tail节点，
tail和尾节点的距离越长使用CAS更新tail节点的次数就会越少，但是距离越长带来的负面效果就是每次入队时定位尾节点的时间就越长，
因为循环体需要多循环一次来定位出尾节点，但是这样仍然能提高入队的效率，
因为从本质上来看它通过增加对volatile变量的读操作来减少了对volatile变量的写操作，而对volatile变量的写操作开销要远远大于读操作
，所以入队效率会有所提升。

在JDK 1.8的实现中，tail的更新时机是通过p和t是否相等来判断的，其实现结果和JDK 1.7相同，即当tail节点和尾节点的距离大于等于1时，更新tail。


#### 出队列
并不是每次出队时都更新head节点，当head节点里有元素时，直接弹出head节点里的元素，而不会更新head节点。
只有当head节点里没有元素时，出队操作才会更新head节点。采用这种方式也是为了减少使用CAS更新head节点的消耗，从而提高出队效率。

```java
public E poll() {
    restartFromHead:
    for (;;) {
        // p节点表示首节点，即需要出队的节点
        for (Node<E> h = head, p = h, q;;) {
            E item = p.item;
 
            // 如果p节点的元素不为null，则通过CAS来设置p节点引用的元素为null，如果成功则返回p节点的元素
            if (item != null && p.casItem(item, null)) {
                // Successful CAS is the linearization point
                // for item to be removed from this queue.
                // 如果p != h，则更新head
                if (p != h) // hop two nodes at a time
                    updateHead(h, ((q = p.next) != null) ? q : p);
                return item;
            }
            // 如果头节点的元素为空或头节点发生了变化，这说明头节点已经被另外一个线程修改了。
            // 那么获取p节点的下一个节点，如果p节点的下一节点为null，则表明队列已经空了
            else if ((q = p.next) == null) {
                // 更新头结点
                updateHead(h, p);
                return null;
            }
            // p == q，则使用新的head重新开始
            else if (p == q)
                continue restartFromHead;
            // 如果下一个元素不为空，则将头节点的下一个节点设置成头节点
            else
                p = q;
        }
    }
}
```

该方法的主要逻辑就是首先获取头节点的元素，然后判断头节点元素是否为空，如果为空，
表示另外一个线程已经进行了一次出队操作将该节点的元素取走，如果不为空，则使用CAS的方式将头节点的引用设置成null，
如果CAS成功，则直接返回头节点的元素，如果不成功，表示另外一个线程已经进行了一次出队操作更新了head节点，导致元素发生了变化，需要重新获取头节点。


